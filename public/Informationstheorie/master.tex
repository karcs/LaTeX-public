\documentclass{article}

\input{../Config/german.tex}
\usepackage[de]{mathenv}
\input{../Config/math.tex}
\usepackage{tikz}
\usepackage{booktabs}

\begin{document}

\person{Gallager}-symmetrisch
schwach-symmetrisch
stark symmetrisch (Zeilen sind Permutationen voneinander) (somit \person{Gallager}- und schwach-symmetrisch)

25
\begin{exercise}
    Die folgenden Kanalmatrizen
    $$
    \begin{pmatrix}
        1/8 & 3/8 & 3/8 & 1/8\\
        3/8 & 3/8 & 3/8 & 1/8
    \end{pmatrix}
    $$
    ist stark symmetrisch.
    $$
    \begin{pmatrix}
        2/3 & 1/3 & 0\\
        0 & 1/3 & 2/3
    \end{pmatrix}
    $$
    schwach und \person{Gallager}-symmetrisch, aber nicht stark.
    $$
    \begin{pmatrix}
        0.7 & 0.2 & 0.1\\
        0.2 & 0.1 & 0.7
    \end{pmatrix}
    $$
    hat keine Symmetrie.
    $$
    \begin{pmatrix}
        0.1 & 0.2 & 0.3 & 0.4\\
        0.2 & 0.1 & 0.4 & 0.3
    \end{pmatrix}
    $$
    \person{Gallager}-symmetrisch.
    $$
    \begin{pmatrix}
        0.3 & 0.2 & 0.5\\
        0.2 & 0.5 & 0.3\\
        0.5 & 0.3 & 0.2
    \end{pmatrix}
    $$
    stark symmetrisch.
    $$
    \begin{pmatrix}
        p & 1-p & 0 & 0\\
        1-p & p & 0 & 0\\
        0 & 0 & q & 1-q\\
        0 & 0 & 1-q & q
    \end{pmatrix}
    $$
    ist stark symmetrisch für $p=1-q$ sonst nichts.
    $$
    \begin{pmatrix}
        0.1 & 0.2 & 0.3 & 0.4\\
        0.2 & 0.4 & 0.3 & 0.1\\
        0.3 & 0.1 & 0.2 & 0.4\\
        0.4 & 0.3 & 0.2 & 0.1
    \end{pmatrix}
    $$
    ist nur schwach symmetrisch.
\end{exercise}

26
\begin{exercise}
    
\end{exercise}

\begin{solution}
    Gegeben ist eine Kanalfolge $K_1$, $K_2$ mit $X_1$ am Eingang und $Y_1$ am Ausgang.
    \begin{tasks}
            \item $$C_1=\max_{p_{X_1}}{\rest{I(X_1,Y_1)}_{p_{X_1}}},$$ dabei sei $p'^\ast_{X_1}$ die optimale Verteilung zu Kanal 1.
        $$C=\rest{I(X_1,Y_2)}_{p^\ast_{X_1}},$$ mit $p^\ast_{X_1}$ die optimale Verteilung für den Gesamtkanal.
        Nun gilt laut Datenverarbeitungsungleichung
        $$
        C = \rest{I(X_1,Y_2)}_{p^\ast_{X_1}} \leq \rest{I(X_1,Y_1)}_{p^\ast_{X_1}}\leq \rest{I(X_1,Y_1)}_{p'^\ast_{X_1}}=C_1 
        $$
            \item Die Kanalmatrix vom Gesamtkanal ermittelt sich zu
        $$
        K = K_1 K_2.
        $$
            \item Gegeben sind zwei symmetrische Binärkanäle (BSC) mit Fehlerwahrscheinlichkeit $\epsilon_1$, Damit
        $$
        K_1 =
        \begin{pmatrix}
            1-\epsilon_1 & \epsilon_1\\
            \epsilon_1 & 1-\epsilon_1
        \end{pmatrix}
        $$
        analog ergibt sich
        $$
        K_2 =
        \begin{pmatrix}
            1-\epsilon_2 & \epsilon_2\\
            \epsilon_2 & 1-\epsilon_2
        \end{pmatrix}.
        $$
        Die Kanalmatrix des Gesamtkanals eine Matrix
        $$
        K = K_1 K_2 =
        \begin{pmatrix}
            1-\epsilon & \epsilon\\
            \epsilon & 1-\epsilon
        \end{pmatrix}
        $$
        mit $\epsilon = \epsilon_1(1-\epsilon_2)+\epsilon_2(1-\epsilon_1)$.
        D.h.~die Gesamtkanalkapazität ist BSC mit Fehlerwahrscheinlichkeit $\epsilon$.
        Also laut Script (3.19)
        $$
        C = 1- H_b(\epsilon).
        $$
            \item Wieder wird ein symmetrischer Binärkanal $K_1$ (BSC) mit einem Binärkanal mit Auslöschung (BEC) hintereinandergeschaltet:
        $$
K_1 =
\begin{pmatrix}
    1-\epsilon_2 & \epsilon_2 & 0\\
    0 & \epsilon_2 & 1-\epsilon_2
\end{pmatrix}
$$
Matrixmultiplikation liefert dann
$$
K =
\begin{pmatrix}
    (1-\epsilon_2)(1-\epsilon_1) & \epsilon_2 & \epsilon_1(1-\epsilon_2)\\
     \epsilon_1(1-\epsilon_2) & \epsilon_2 & (1-\epsilon_2)(1-\epsilon_1)
\end{pmatrix}.
$$

Diese ist \person{Gallager}-symmetrisch, denn die Zeilen sind Permutationen voneinander und es existiert eine Zerlegung in stark symmetrische Matrizen. Die optimale Eingangsverteilung ist also die Gleichverteilung.
Es berechnet sich dann $I(X_1,Y_2)=H(Y_2)-H(Y_2|X_1)$ für $X_1$ gleichverteilt. Es ergibt sich
$$
\begin{matrix}
    Y_1 & 0 & \Delta & 1 \\
    \midrule
    p_{Y_2}(y_2) & \frac 1 2(1-\epsilon_2) & \epsilon_2 & \frac 1 2 (1-\epsilon_2)
\end{matrix}
$$
also $H(Y_2)=(1-\epsilon_2)+H_b(\epsilon_2)$ und $H(Y_2|Y_1)=\frac 1 2 H(Y_2|X_1=0) + \frac 1 2 H(Y_2|X_1=1)=(1-\epsilon_2)H_b(\epsilon_1)+H_b(\epsilon_2)$. Also
$$C=(1-\epsilon_2)(1-H_b(\epsilon_1)).$$
Bei $\epsilon_1=0$ bleibt nur der BEC über, bei $\epsilon_2=0$ nur der BSC.
    \end{tasks}
\end{solution}
27
\begin{solution}
    \begin{tasks}
            \item Die Kanalmatrix für den Gesamtkanal ist zu ermitteln. Diese ergibt sich zu
        $$
        K = \alpha K_1 + (1-\alpha) K_2,
        $$
        da mit Wahrscheinlichkeit $\alpha$ der Kanal $K_1$ und mit $1-\alpha$ Kanal $K_2$ durchlaufen wird.
            \item Für feste Eingangsverteilung ist die Transinformation $I$ konvex bzgl. der Kanalmatrix (1.24), d.h.
        $$
        I(p_X,\alpha K_1+(1-\alpha)K_2)\leq \alpha I(p_X,K_1)+(1-\alpha)I(p_x,K_2).
        $$
        mit $I(X,Y)=:I(p_X,K)$.
        Sei $p_X^{(1)\ast}$ optimale Verteilung zu $K_1$ und
        $p_X^{(2)\ast}$ optimale Verteilung zu $K_2$, sowie
        $p^\ast$ optimale Verteilung zu $K$.
        Daraus folgt
        $$
        \eqalign{
            C & = I(p^\ast_X)\leq \alpha I(p^ast_X,K_1)+(1-\alpha) I(p^\ast_X,K_2)\cr
            & \leq \alpha I(p^{(1)\ast}_X,K_1)+(1-\alpha)I(p^{(2)\ast}_X,K_2)\cr
            & =\alpha C_1+(1-\alpha)C_2,}
        $$
        was die gesuchte Identität ist nach Definition der Informationskapazität.
            \item Kanalmatrix für den Gesamtkanal (wieder zwei BSC's hintereinandergeschaltet) ergibt sich zu
        $$
        K = \alpha
        \begin{pmatrix}
            1-\epsilon_1 & \epsilon_1\\
            \epsilon_1 & 1-\epsilon_1
        \end{pmatrix}
        +(1-\alpha)
        \begin{pmatrix}
            1-\epsilon_2 & \epsilon_2\\
            \epsilon_2 & 1-\epsilon_2
        \end{pmatrix}=
        \begin{pmatrix}
            1-\epsilon & \epsilon\\
            \epsilon & 1-\epsilon
        \end{pmatrix}
        $$
        mit $\epsilon=\alpha\epsilon_1+(1-\alpha)\epsilon_2$. D.h. der Gesamtkanal ist BSC mit Fehlerwahrscheinlichkeit $\epsilon$, damit laut Script $C = 1-H_b(\epsilon)$.
    \end{tasks}
\end{solution}

\section{4. Hausaufgabe zur LV Informationstheorie (28-30)}\hfill{19.06.2014}
\begin{exercise}[Stark- und schwach typische Sequenzen diskreter gedächtnisloser Quellen]
    Gegeben ist eine diskrete Gedächtnislose Quelle $(U_k)_k$ mit einem binären Alphabet $\cU=\set{0,1}$. Die Auftrittswahrscheinlichkeit für das Symbol $0$ sei $q$.
    \begin{align*}
        s_1&=(00100)\\
        s_2&=(10101)\\
        s_3&=(10111)\\
        s_4&=(11111)
    \end{align*}
    Entscheiden Sie für jede dieser Sequenzen, ob die Sequenz $\epsilon$-stark und oder $\epsilon$-schwach-typisch ist, wenn $\epsilon$ und $q$ folgende Werte haben.
    \begin{tasks}
            \item $\epsilon=0,15$, $q=1/3$.
            \item $\epsilon=0,3$, $q=1/7$.
    \end{tasks}
\end{exercise}

\begin{remark}
    Die Sequenz ist \emph{schwach typisch}, wenn sie die Ungleichung $\abs{-\frac 1 n \log_2(p_{\cU}^{(n)}(u^{(n)})-H(U)}\leq \epsilon$ erfüllt, der erste Term heißt dabei empirische Entropie (Bezeichnung $\hat {H(U)}^{(n)}$). Ist die Sequenz stark typisch, so muss die Ungleichung $\abs{\frac 1 n N(a|u^{(n)})-p_U(a)}<\epsilon/\abs{U}$ für alle $a\in\cU$.
\end{remark}

\begin{solution}
    Es gilt $p_U(s_1)=q^3{(1-q)}^2$, $p_U(s_2)=q^2{(1-q)}^3$, $p_U(s_3)=q{(1-q)}^4$, $p_U(s_4)={(1-q)}^5$.
    \begin{tasks}
            \item $\epsilon=0,15$, $q=1/3$, $p_U(s_1)=4/243$, $p_U(s_2)=8/243$, $p_U(s_3)=16/243$, $p_U(s_4)=32/243$.
        Damit erhält man $\hat H(s_1)=1,185$bit, $\hat H(s_2)=0,985$bit, $\hat H(s_3)=0,785$bit und $\hat H(s_4)=$. Es ergibt sich also:
        \begin{align*}
            \abs{\hat H(s_1)-H(U)}&=0,267\\
            \abs{\hat H(s_2)-H(U)}&=0,067\\
            \abs{\hat H(s_3)-H(U)}&=0,133\\
            \abs{\hat H(s_4)-H(U)}&=0,333
        \end{align*}
    
    Damit ist der Abstand für $s_2, s_3$ kleiner als $\epsilon=0,15$ also sind sie schwach typisch.
    Nun testen wir auf start-typisch.
    \begin{align*}
        \begin{matrix}
            s_i & N(0|s_i) & N(1|s_i) & \abs{1/n N(a|s_i)-p_U(a)}\\
            s_1 & 3 & 2 & \abs{3/5-1/3}=0,2667\\ 
            s_2 & 2 & 3 & \abs{2/5-1/3}=0,067\\
            s_3 & 1 & 4 & \abs{1/5-1/3}=0,133\\
            s_4 & 0 & 5 & 1/3
        \end{matrix}
    \end{align*}
    Damit sind $s_2,s_3$ stark- und und schwach-typisch. $s_1,s_4$ sind weder das eine noch das andere.
    \begin{remark}
        Im binären gilt $\abs{1/n N(0|s)-p_U(0)}=\abs{1/n N(1|s)-p_U(1)}$, da $\abs{1/nN(1|s)-p_U(1)}=\abs{1/n(n-N(1|s))-(1-p_U(0))}=\abs{1/n N(0|s)-p_U(0)}$.
    \end{remark}
    \item Analog berechnet man, dass hier $s_3$ stark und schwach typisch ist, $s_4$ nur stark typisch ist und $s_1,s_2$ beides nicht sind.
\begin{remark}
    Es sind also alle Kombinationen aus stark und schwach typisch möglich.
\end{remark}
\end{tasks}
\end{solution}

\begin{exercise}[Informationskapazität]
    Gegeben seien zwei unabhängige Zufallsvariablen $X$ und $Z$ mit Alphabeten $\cX=\set{1,2,3}$. Die Zufallsgröße $Z$ besitzt die Wahrscheinlichkeitsdichte $p_Z$, die wie folgt gegeben ist:
    $$
    \begin{matrix}
        z & 1 & 2 & 3\\
        \midrule
        p_z(\delta) & \epsilon & \delta & 1 -\delta-\epsilon
    \end{matrix}.
    $$
    Es soll $\epsilon,\delta\in[0,1/2]$ gelten. Es sei $Y$ durch $Y:=X+Z \mod 3$ gegeben.
    \begin{tasks}
            \item Gib das Alphabet von $Y$ an.
            \item Bestimme die Wahrscheinlichkeiten $p_{Y|X}(\arg|x)$ für alle $x\in\cX$.
            \item Wir wollen nun $X$ und $Y$ als Eingang bzw.~Ausgang eineds DMC betrachten. Berechnen Sie die Informationskapazität diese DMC in Abhängigkeit von $\epsilon, \delta$ und geben Sie die zugehörige optimale Wahrscheinlichkeitsverteilung an.
        \item Sei $\epsilon=\delta$, für welches $\epsilon$ nimmt nun die in der vorherigen Teilaufgabe berechnete Informationskapazität ihr Minimum und Maximum an.
    \end{tasks}
\end{exercise}

\begin{solution}
    \begin{tasks}
        \item $\cY=\cX$.
            \item Folgende Tabelle ergibt sich:
        $$
        \begin{matrix}
            X & Z & X+Z & Y & P_{Y|X}(y|x)\\
            \midrule
            1 & 1 & 2 & 2 & \delta\\
            1 & 2 & 3 & 2 & \epsilon\\
            1 & 3 & 4 & 1 & 1-\delta-\epsilon\\
            2 & 1 & 3 & 0 & \delta\\
            2 & 2 & 4 & 1 & \epsilon\\
            2 & 3 & 5 & 2 & 1-\delta-\epsilon\\
            3 & 1 & 4 & 1 & \delta\\
            3 & 2 & 5 & 2 & \epsilon\\
            3 & 3 & 6 & 0 & 1-\delta-\epsilon
        \end{matrix}
        $$
        Man hat also die Kanalmatrix:
        $$
        \begin{matrix}
            p_{Y|X} (x,y) & 0 & 1 & 2\\
            1 & \epsilon & 1-\delta-\epsilon & \delta\\
            2 & \delta & \epsilon & 1-\delta-\epsilon\\
            3 & 1-\delta-\epsilon & \delta & \epsilon
        \end{matrix}.
        $$
            \item Der DMC ist symmetrisch. Die Informationskapazität wird durch Gleichverteilung am Eingang erreicht (Skript §3.21). Für die Kapazität gilt (§3.22)
        $$
        C = \log_2\card\cY - H(r)
        $$
        hier also:
        $$
        C = \log_2 3 - (-\epsilon\log_2\epsilon-\delta\log\delta-(1-\delta-\epsilon)\log_2(1-\delta-\epsilon).
        $$
        \item Hier $\epsilon=\delta$. Dann wird die Entropie maximal bei Gleichverteilung, die Kapazität also minimal für $\epsilon=\delta=1/3$.
    $C_{\min}=0$. Die Entropie wird minimal für $\epsilon=0$, dann ist $C_{\max}=\log_2 3$.
\end{tasks}
\end{solution}

\begin{exercise}[Hintereinanderschaltung von Multiplikationskanälen]
    Gegeben sei ein diskreter gedächtnisloser Multiplikationskanal mit binärem Ein- und Ausgang ($X_1, Y_1$).
    Das multiplikative Rauschen wird durch die vom Kanaleingang unabhängige zufallsgröße $Z_1$ mit binärem Alphabet $\cZ_1=\set{0,1}$ beschrieben, wobei $P(Z_1=1)=\epsilon_1$ sei.
    \begin{tasks}
        \item Bestimme die Kanalmatrix und die Informationskapazität.
    \end{tasks}
\end{exercise}

\begin{solution}
    \begin{tasks}
            \item Siehe Aufgabe 16:
        $$
        K_1=
        \begin{pmatrix}
            1 & 0\\
            1-\epsilon_1 & \epsilon_1
        \end{pmatrix}
        $$
        Es gilt $C_1 = \max_{p_X}{I(x;y_1)} = \log_2(1+\epsilon_1{(1-\epsilon_1)}^{\frac{1-\epsilon_1}{\epsilon_1}})$bit mit
            $$
            p_X = \frac 1 {{(1-\epsilon_1)}^{\frac{\epsilon_1-1}{\epsilon_1}}+\epsilon_1}.
            $$
                \item Durch Hintereinanderschaltung zweier Kanäle entsteht die Gesamtkanalmatrix durch Multiplikation der beiden Teilkanalmatrizen ($K=K_1K_2$).
            $$
            K =
            \begin{pmatrix}
                1 & 0 \\
                1-\epsilon_1 & \epsilon_1
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 \\
                1-\epsilon_2 & \epsilon_2
            \end{pmatrix}
            =
            \begin{pmatrix}
                1 & 0 \\
                1-\epsilon & \epsilon
            \end{pmatrix}
            $$
        \end{tasks}
        mit $\epsilon=\epsilon_1\epsilon_2$. Gesamtkapazität berechnet sich nach derselben Formel
        $$
        C_{\text{ges}} = \log_2(1+\epsilon{(1-\epsilon)}^{\frac{1-\epsilon}{\epsilon}})
        $$
        und
        $$
        p_X = \frac 1 {{(1-\epsilon)}^{\epsilon-1}{\epsilon}+\epsilon}.
        $$
        Der Gesamtkanal ist wieder ein $Z$-Kanal mit Parameter $\epsilon=\epsilon_1\epsilon_2$.
\end{solution}

\hfill{26.06.14}
% 31
\begin{exercise}
    
\end{exercise}

\begin{solution}
    \begin{tasks}
            \item Der Erwartungswert der Zufallsgröße ist $0$, da die Verteilung symmetrisch um $0$ ist, d.h. $f(x)=\frac 1 a\left(1-\abs{\frac x a}\right)$. Die Varianz bestimmen wir, indem wir $E({(X-E(X))}^2)$ berechnen, dann erhalten wir also
        $$
        \int_{-a}^a{\frac {x^2} a\left(1-\abs{\frac x a}\right)}\diff x = 2a^2\int_0^1{y^2(1-y)}\diff y =\frac {a^2} 6. 
        $$
            \item Gegeben ist eine Zufallsgröße $X$ mit und $Y=g(X)$ mit $f_X$ Dichte von $X$ und $f_Y$ Dichte von $Y$. Es sei $g(x)=\exp(x/a)$. Der Transformationssatz für Dichten besagt:
        $$
        f_Y(y)=\frac{f_X(g^{-1}(y))}{\abs{g'(g^{-1}(y))}}
        $$
        Ableitung ist dann $g'(x)=\frac 1 a \exp(x/a)$, Umkehrfunktion $g^{-1}(y)=a\log(y)$ ($y\in (0,\infty)$). Träger von $Y$ ist $\bar{g(-a,a)}=[e^{-1},e]$. Also für $y\in[e^{-1},e]$ gilt:
        $$
        f_Y(y)=\frac{\frac 1 a\left(1-\abs{\frac{a\log(y)}{a}}\right)}{\frac 1 a\exp\left(\frac{a\log(y)}{a}\right)}
        $$
        insgesamt
        $$
        f_Y(y)=
        \begin{cases}
            \frac 1 y\left(1-\abs{\log(y)}\right) &: y\in [e^{-1},e]\\
            0 &: \text{sonst}
        \end{cases}
        $$
            \item Es ist
        \begin{align*}
            h(X) &
            =-\int_{-\infty}^\infty{\log_2(f_X(x))f_X(x)}\\
            &
            = -\int_{-a}^a{\frac 1 a\left(1-\abs{\frac x a}\right)\log_2\left(\frac 1 a\left(1-\abs{\frac x a}\right)\right)}\\
            &
            = 2a^2\int_{y=1/a}^0{y\log_2(y)}\diff y\\
            &
            = \rest{\frac{-2a^2}{\log(2)}\left(y^2\left(\frac{\log(y)}{4}-\frac 1 4\right)\right)}_{1/a}^0\\
            & = \log_2(\sqrt e a)
        \end{align*}
    \end{tasks}
\end{solution}

\begin{solution}
    Die Dichten von $X$ und $Y$ sind wie folgt gegeben durch
    $$
    f_X(x)=\frac 1 {\sqrt{2\pi\sigma_X^2}}\exp(-{(\frac x {\sigma_X})}^2)
        $$ und $f_Y$ analog.
        Dann ist
        \begin{align*}
            D(X||Y) &= \int_{-\infty}^\infty{f_X(x)\log_2\left(\frac{f_X(x)}{f_Y(x)}\right)}\diff x\\
            & = \int_{-\infty}^\infty{\frac 1 {\sqrt{2\pi\sigma_X^2}}\exp\left(-\frac 1 2{\left(\frac x {\sigma_X}\right)}^2\right)\log_2\left(\sqrt{\frac{\sigma_Y^2}{\sigma_X^2}}\exp\left(-\frac 1 2\left(\frac 1 {\sigma_X^2}-\frac 1 {\sigma_Y^2}\right)x^2\right)\right)}\diff x\\
            & =
            \frac 1 2\log_2\left(\frac{\sigma_Y^2}{\sigma_X^2}\right)E(X)+\frac 1{\log(2)}\frac 1 2\left(\frac 1 {\sigma_Y^2}-\frac 1 {\sigma_X^2}\right)V(X)\\
            & = \frac 1 {2\log(2)}\left(2\log\left(\frac{\sigma_Y}{\sigma_X}\right)+\left(\frac{\sigma_X^2}{\sigma_Y^2}-1\right)\right)
        \end{align*}
\end{solution}

% 33
\begin{solution}
    Wir haben $Z_1:=X_1+Y$, $Z_2:=X_2+Y$, $Z_1$ ist normalverteilt, denn die Summe normalverteilter Zufallsgrößen ist normalverteilt. $E(Z_1)=E(X_1)+E(Y)=0+0=0$ und die Varianz ist $V(Z_1)=V(X_1)+V(Y)=\sigma_1^2+\sigma_Y^2$, da $X_1, Y$ unkorreliert (da unabhängig).
    Entsprechend ist $E(Z_2)=0$ und $V(Z_2)=\sigma_2^2+\sigma_Y^2$.
    \begin{tasks}
            \item Mit Aufgabe 32 erhalten wir:
        $$
        D(X_1+Y||X_2+Y)=\frac 1 {2\log(2)}\left(\log\left(\frac{\sigma_2^2+\sigma_Y^2}{\sigma_1^2+\sigma_Y^2}\right)+\frac{\sigma_1^2+\sigma_Y^2}{\sigma_2^2+\sigma_Y^2}-1\right)
        $$
            \item Es gilt $D(X_1||X_2)\geq D(X_1+Y||X_2+Y)$, da
        $$
        \log\left(\frac{b+x}{a+x}\right)+\frac{a+x}{b+x}-1
        $$
        monoton fallend in $x$ ist.
        Die Ableitung ist
        $$
        \frac{a+x}{b+x}\frac{a-b}{{(a+x)}^2}+\frac{b-a}{{(b+x)}^2} = \frac{a-b}{b+x}\left(\frac1 {a+x}-\frac 1 {b+x}\right)=\frac{-{(b-a)}^2}{ {(b+x)}^2(a+x)}<0
        $$
    \end{tasks}
    \begin{remark}
        Durch additive Störung werden die Verteilungen der Zufallsgrößen $X_1$ und $X_2$ ähnlicher, daher $D(\arg || \arg)$ kleiner.
    \end{remark}
\end{solution}

% 34
\begin{solution}
    Äquivalentes Modell: $\tilde Z = Z_1+Z_2$, $\tilde Z$ normalverteilt, und $Y=X+\tilde Z$.
    \begin{tasks}
            \item $E(\tilde Z)=E(Z_1)+E(Z_2)=0$ und $V(\tilde Z)=V(Z_1)+V(Z_2)=\sigma_1^2+\sigma_2^2$ (da $Z_1$ und $Z_2$ unabhängig sind.
        $X$ und $(Z_1,Z_2)$ sind unabhängig, also Modell \person{Gauß}-Kanal (Hintereinanderschaltung von zwei \person{Gauß}-Kanälen).
        $$
        \max_{E(X^2)\leq P}{I(X;Y)} = \max_{E(X^2)\leq P}{I(X;X+Z)} = \frac 1 2\log_2\left(1+\frac{P}{\sigma_1^2+\sigma_2^2})
            $$ nach der Formel für die Informationskapazität des \person{Gauß}-Kanals (§3.51).
                \item Es ergibt sich
            \begin{align*}
                V(\tilde Z) &= V(Z_1)+2C(Z_1,Z_2)+V(Z_2)\\
                &=\sigma_1^2+2\alpha\sigma_1\sigma_2+\sigma_2^2
            \end{align*}
            Wieder muss in die Formel für die Transinformation eingesetzt werden.
        \end{tasks}
\end{solution}
\end{document}






